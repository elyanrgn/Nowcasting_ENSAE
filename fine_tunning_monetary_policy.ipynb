{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638f315a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "def prepare_monetary_sentiment_dataset():\n",
    "    \"\"\"\n",
    "    Create training dataset with monetary policy labels\n",
    "    Label: \n",
    "    - 0 = dovish (expansionary/low inflation risk)\n",
    "    - 1 = neutral\n",
    "    - 2 = hawkish (contractionary/high inflation risk)\n",
    "    \"\"\"\n",
    "    \n",
    "    data = {\n",
    "        'text': [\n",
    "            \"Energy prices are expected to rise\",\n",
    "            \"Unemployment rate decreased to 3.5%\",\n",
    "            \"Core inflation remains stable at target\",\n",
    "            \"Wage growth accelerates beyond expectations\",\n",
    "            \"Economic growth slows to 1.2%\",\n",
    "            \"Oil prices decline sharply\",\n",
    "            \"Central bank maintains rates unchanged\"\n",
    "        ],\n",
    "        'label': [\n",
    "            2,  # Hawkish - inflation pressure\n",
    "            2,  # Hawkish - tight labor market\n",
    "            1,  # Neutral - stable inflation\n",
    "            2,  # Hawkish - wage-price spiral risk\n",
    "            0,  # Dovish - weak growth\n",
    "            0,  # Dovish - disinflationary\n",
    "            1   # Neutral - no change\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return Dataset.from_pandas(pd.DataFrame(data))\n",
    "\n",
    "def fine_tune_monetary_finbert(train_dataset, output_dir='./monetary-finbert'):\n",
    "    \"\"\"\n",
    "    Fine-tune FinBERT for monetary policy sentiment\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load pre-trained FinBERT\n",
    "    model_name = 'ProsusAI/finbert'\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        model_name, \n",
    "        num_labels=3,  # dovish, neutral, hawkish\n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "    \n",
    "    # Tokenize dataset\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples['text'],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=128\n",
    "        )\n",
    "    \n",
    "    tokenized_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        warmup_steps=100,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        learning_rate=2e-5\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    # Save model\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "# Usage\n",
    "# dataset = prepare_monetary_sentiment_dataset()\n",
    "# model, tokenizer = fine_tune_monetary_finbert(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4da0c08e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'is_numba_available' from 'transformers.utils' (c:\\Users\\elyan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[32m      4\u001b[39m pipeline = pipeline(\u001b[33m\"\u001b[39m\u001b[33mfill-mask\u001b[39m\u001b[33m\"\u001b[39m, model=\u001b[33m\"\u001b[39m\u001b[33mcamembert-base\u001b[39m\u001b[33m\"\u001b[39m, dtype=torch.float16, device=\u001b[32m0\u001b[39m)\n\u001b[32m      5\u001b[39m pipeline(\u001b[33m\"\u001b[39m\u001b[33mLe camembert est un délicieux fromage <mask>.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elyan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\__init__.py:31\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     32\u001b[39m     OptionalDependencyNotAvailable,\n\u001b[32m     33\u001b[39m     _LazyModule,\n\u001b[32m     34\u001b[39m     is_essentia_available,\n\u001b[32m     35\u001b[39m     is_g2p_en_available,\n\u001b[32m     36\u001b[39m     is_librosa_available,\n\u001b[32m     37\u001b[39m     is_mistral_common_available,\n\u001b[32m     38\u001b[39m     is_mlx_available,\n\u001b[32m     39\u001b[39m     is_numba_available,\n\u001b[32m     40\u001b[39m     is_pretty_midi_available,\n\u001b[32m     41\u001b[39m )\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# Note: the following symbols are deliberately exported with `as`\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# so that mypy, pylint or other static linters can recognize them,\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# given that they are not exported using `__all__` in this file.\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_bitsandbytes_available \u001b[38;5;28;01mas\u001b[39;00m is_bitsandbytes_available\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'is_numba_available' from 'transformers.utils' (c:\\Users\\elyan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "pipeline = pipeline(\"fill-mask\", model=\"camembert-base\", dtype=torch.float16, device=0)\n",
    "pipeline(\"Le camembert est un délicieux fromage <mask>.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eaf734d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting huggingface-hub==0.34\n",
      "  Downloading huggingface_hub-0.34.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\elyan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub==0.34) (3.20.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\elyan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub==0.34) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\elyan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub==0.34) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\elyan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub==0.34) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\elyan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub==0.34) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\elyan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub==0.34) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\elyan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub==0.34) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\elyan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub==0.34) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\elyan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub==0.34) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\elyan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub==0.34) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\elyan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub==0.34) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\elyan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub==0.34) (2025.6.15)\n",
      "Downloading huggingface_hub-0.34.0-py3-none-any.whl (558 kB)\n",
      "   ---------------------------------------- 0.0/558.7 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 524.3/558.7 kB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 558.7/558.7 kB 3.8 MB/s  0:00:00\n",
      "Installing collected packages: huggingface-hub\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface_hub 1.4.1\n",
      "    Uninstalling huggingface_hub-1.4.1:\n",
      "      Successfully uninstalled huggingface_hub-1.4.1\n",
      "Successfully installed huggingface-hub-0.34.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "mantis-tsfm 0.2.0 requires huggingface-hub<0.24,>=0.23, but you have huggingface-hub 0.34.0 which is incompatible.\n",
      "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 5.1.0 which is incompatible.\n",
      "transformers 5.1.0 requires huggingface-hub<2.0,>=1.3.0, but you have huggingface-hub 0.34.0 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install huggingface-hub==0.34"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
